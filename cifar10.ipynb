{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1URubJ1dyzWyF_ye_G_13Q2RqM2pCgPEP",
      "authorship_tag": "ABX9TyMXm+1RkTScFs32Q2M/E5zI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeynep68/deep_learning/blob/master/cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWd-R9LggVRh",
        "colab_type": "code",
        "outputId": "c18be86e-b095-4328-fc47-485eddee026f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import Model \n",
        "from tensorflow.keras.optimizers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH5F6Mhygcb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CIFAR10():\n",
        "    def __init__(self):\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "        self.width = self.x_train.shape[1]\n",
        "        self.height = self.x_train.shape[2]\n",
        "        self.depth = self.x_train.shape[3]\n",
        "        \n",
        "        self.num_classes = np.max(self.y_train) + 1\n",
        "        \n",
        "        self.train_size = self.x_train.shape[0]\n",
        "        self.test_size = self.x_test.shape[0]\n",
        "        \n",
        "        self.y_train = to_categorical(self.y_train, self.num_classes)\n",
        "        self.y_test = to_categorical(self.y_test, self.num_classes)\n",
        "\n",
        "        self.scaler = StandardScaler()    \n",
        "\n",
        "    def get_train_set(self):\n",
        "        return self.x_train, self.y_train\n",
        "    \n",
        "    def get_test_set(self):\n",
        "        return self.x_test, self.y_test\n",
        "\n",
        "    def normalize_data(self):\n",
        "        self.scaler.fit(self.x_train.reshape(-1, 1024))\n",
        "    \n",
        "        self.x_train = self.scaler.transform(self.x_train.reshape(-1, 1024))\n",
        "        self.x_test = self.scaler.transform(self.x_test.reshape(-1,1024))\n",
        "\n",
        "        self.x_train = self.x_train.reshape((self.train_size, self.width, self.height, self.depth))\n",
        "        self.x_test = self.x_test.reshape((self.test_size, self.width, self.height, self.depth))\n",
        "        \n",
        "    def data_augmentation(self, size=5000):\n",
        "        img_generator = ImageDataGenerator(\n",
        "                            rotation_range=15,\n",
        "                            zoom_range=0.05,\n",
        "                            width_shift_range=0.1,\n",
        "                            height_shift_range=0.1,\n",
        "                            brightness_range=None,\n",
        "                            horizontal_flip=False,\n",
        "                            vertical_flip=False\n",
        "                            )\n",
        "        img_generator.fit(self.x_train, augment=True) \n",
        "\n",
        "        rand_numbers = np.random.randint(self.train_size, size=size)\n",
        "        x_augmented = np.zeros(shape=(1,self.width, self.height, self.depth))\n",
        "        y_augmented = np.zeros(shape=(1,self.num_classes))\n",
        "\n",
        "        for i in range(size):\n",
        "            x_augmented = np.vstack((x_augmented, self.x_train[ rand_numbers[i] ].reshape(1,self.width, self.height, self.depth)))\n",
        "            y_augmented = np.vstack( (y_augmented, self.y_train[rand_numbers[i]].reshape(1,self.num_classes) ) )\n",
        "            \n",
        "        x_augmented = np.delete(x_augmented, 0, axis=0)\n",
        "        y_augmented = np.delete(y_augmented, 0, axis=0)\n",
        "\n",
        "        x_augmented = img_generator.flow(x_augmented, np.zeros(size), batch_size=size, shuffle=False).next()[0] # next  \n",
        "\n",
        "        self.x_train = np.concatenate((self.x_train, x_augmented))\n",
        "        self.y_train = np.concatenate((self.y_train, y_augmented))\n",
        "\n",
        "        self.train_size = self.x_train.shape[0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65IxW2hLgfJG",
        "colab_type": "code",
        "outputId": "75eb37f3-2c49-4554-d8bb-fc67c718e31a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    cifar10 = CIFAR10()\n",
        "    img = cifar10.x_train[1]\n",
        "    plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAf8ElEQVR4nO2dW5BdZ5Xf/+vc+n5vdasltdSSLAkZ\n+YpQbOwAGQI2hJShZuKCB8IDNZ5KQSVUJg8upiqQqjwwqQDFQ0LKBNeYCcGQAQaXYTJ4jAfDGNvI\nN1mybFnWXepuXVunL+d+Vh7OcZXsfP+v25L6tJj9/1WpdPpb/e29zt577X36+5+1lrk7hBD/+Emt\ntANCiNagYBciISjYhUgICnYhEoKCXYiEoGAXIiFkrmSymd0N4JsA0gD+p7t/Nfb7Pb19PjQyGrSV\niwt0XrVcDI67G52TzbVTW66N29LZHLWlUuH9FQtzdE65VKA2r9WozcDfWyqd5vNS4ft3V3cPndMW\nOR5eq1JbocDPGRCWdOtepzOKBX6sahE/YvIxM1Wr3I96PbY9Pi+T4eGUyfBz5ghfBzFVvE7cKCwU\nUCqVgxfPZQe7maUB/DcAHwZwAsDvzOwRd3+FzRkaGcWfff2/B20nXn2O7uvM4f3B8VqNuz+6/l3U\ntn7zdmobWL2e2to7wvs7sO8pOufowT3UVpnlN4l05L31DvRRW6a9Mzi+64730znXbeXHqnjxPLXt\n2/sCtdXr5eB4uRK+cQPAK/teprb8zFlqK5VL1FYph4Ps/Dl+o5pb4D5Wa3xfq1YNUtvAYDe11Xw2\nvK8KnYJiIXwn+PsnnqZzruRj/C4AB939kLuXATwM4J4r2J4QYhm5kmBfC+D4JT+faI4JIa5Bln2B\nzszuM7PdZrZ7Nn9xuXcnhCBcSbCfBDB+yc/rmmNvwd0fcPed7r6zp5f/rSmEWF6uJNh/B2CLmW00\nsxyATwF45Oq4JYS42lz2ary7V83sCwD+Fg3p7UF33xebU6vVkL8QXt0d6ucrmb4qLNd5ppfOGVu/\niftR58ucqTpfpa0vhOWf4oVzdI4X+Mru2uERals/fh21jV+3gdrWrF0XHB8hkicAZLNt1FbtD6/u\nA8D4utV8XjW8Gl8scnlt5gJXJ86e5apAJiKzwsKr8QND/D23d3EfL+YvUFtbOw+nunPpMJsJ+5K/\nOEPnlEvh1XhnmhyuUGd3958D+PmVbEMI0Rr0DTohEoKCXYiEoGAXIiEo2IVICAp2IRLCFa3Gv2Pc\ngUpY9iqXuBy2sBCWcSa28m/nzs3PU1ssGWNwOJJkkg3fG7ds2UrnvO+2ndS2djQskwFAX98qaqtk\neLZcZ3tYxslEMqisGslsm+dyWImcSwDo7AhLdgP9XG7cvOl6atu//zVqg3E/SqWwlNrXO0DnRBIf\ncTE/TW2O8HUKxDPpLlwIX6uFBZ50wzLiYhmAerILkRAU7EIkBAW7EAlBwS5EQlCwC5EQWroa7/U6\nqiQRwqp8hbkt1xEcv3iWlyoaWs1Xute/myeZjIyvobYsW6aN1A+qVPnK/6uTPIFm4dAZvs0UX/V9\n7eWXguPv3c5Xut+/673UFlvdzUfqExw7eio4nstGagPmeGLT8CquvBw7/jrfJinTNVfgak0+z6+r\nTJbXBuzt5UlDsXp9rLxerE5eW1v4WjTunp7sQiQFBbsQCUHBLkRCULALkRAU7EIkBAW7EAmh5dJb\naSEseXR3cEmmdzCcFHLrTTfTOeObtlDbbCTx47VDx6ktvxCWT+ZmeK2wczNcXpuc4vXMeiOJMEjx\nBIlHf/Cj4Hj2Xn5f/8Dtd1JbNstlxdWruUwJD8tXMxfC3U8A4PkXePecTKROXlcPl+yqtbB0WJ7j\n5ywdeQTGur7UalwSPXeey3kphCW7WDup/v5wwlY60mZKT3YhEoKCXYiEoGAXIiEo2IVICAp2IRKC\ngl2IhHBF0puZHQEwC6AGoOruvOAaAEsZ2tqyQVsl3UPnFTrCjewP53mbnhd/8yy1nT/H66qdPMVr\njGXT4ZSibIpnJ5VIGyQAKBa5bWwVPzWnp45SWy/JhpqdydM5Bw4f5n6MDVNbNst9HBsPt4ZaQ8YB\n4NgUlz1fe5nbRsa4THnkGJG8Kvyc1cvcVovU/2vPcXmwLRO+7gGgUAxvs7eXS4oZ0jLKIs/vq6Gz\n/zN3IqoKIa4Z9DFeiIRwpcHuAH5hZs+Z2X1XwyEhxPJwpR/j73T3k2Y2AuAxM3vV3Z+89BeaN4H7\nAKB/gH/VUAixvFzRk93dTzb/Pw3gJwB2BX7nAXff6e47u7rDC21CiOXnsoPdzLrMrOfN1wA+AmDv\n1XJMCHF1uZKP8aMAfmKNCncZAP/b3f9vbEIqlUFn52jQdnqGZ6IdPB6WXV7Zx+8tqYgsVIu0mirM\n8kKEaSKxFUpc1pqZ5bbZSGulIyf2U1tXB5cpt23eFjZEJMB/+PXfU9uGjRupbes23vZqaCicldXW\nzs9LXy+XrlJVXtxyvsSfWayFUmGGZ9/VarxIaHsHl9Dm8nybvZHMvLb2cKZauRxriRbOwKzXuWx4\n2cHu7ocA3HS584UQrUXSmxAJQcEuREJQsAuREBTsQiQEBbsQCaGlBSfT6Qz6B8NZVAePH6DzJo+E\ns7I6s7zw4sV5XsxxLn+a2iwiXczMhqWymQKXajIkyw8AhkdHqK2jJyxdAcDaCS6CjBMZ5/BLv6Vz\n0sZluUqNZ3mdOcuLad5ww/bg+HVbNtE545Hste7bbqG2Pa8eo7ZSMVzItJSNZL2By2R15xLx1FS4\nvx0A5Nq4rNg3wK4DLgMXCuGMz7rz96UnuxAJQcEuREJQsAuREBTsQiQEBbsQCaGlq/Gl0jzeeCNc\nG+7VNw7Seacm3wiO1yJJKz19XdS2bcsEte3YvoPaJs+EV0CPnuF+rFodTvwBgA2beZJJzxBfqZ++\nwPfnZ8PKxbGjfMX6TKRF1fbrqQkf3hpecQeA+TmyWswX9+Flrgrse5qrCVu28TZgo2v7g+NPP/tk\ncBwApqZ58lKlwlfjiwXu/4VI26uO7rCPsZX1edJGLZYIoye7EAlBwS5EQlCwC5EQFOxCJAQFuxAJ\nQcEuREJoqfQ2P5fH008+FnZklNROA7B5+w3B8Y5Im57t12+htm1b11FbrRhOJAEAT4XlpHnwhjiZ\nbDgRAwDS6bDkAgCVKk+cmJ89T2195bA0VK05nXPsNE8aau8+yffVO0BtmzZPBMc98nwpzITrqgHA\nq8+8SG1e4NfBjrvuDo7fcCNPyCns5tLbGwePUFtnJ6+e3Nc/RG2N7mn/P/k8Py+lUvhYuaQ3IYSC\nXYiEoGAXIiEo2IVICAp2IRKCgl2IhLCo9GZmDwL4OIDT7r6jOTYI4AcAJgAcAXCvu3OdoEmlXMXp\n42GZ6pab/gWd19YWrk02yFUyjK3hdcTOR1r/HD/IZa1yPSyHpYyncqUzXAqpOa+hh2qsfVVYAgQA\nr4X3190Xrv0HAOfmeBZdKsezB+vO5bxGN+/QJD6ju52fs4k149TWnuZ+pBCuG3jDDp5x2N/PJdFH\nCr+gtqlJHgJrR9ZQW83CNQyzkRZm+XxYHtyfDbdKA5b2ZP8LAG8XK+8H8Li7bwHwePNnIcQ1zKLB\n3uy3/vbH3T0AHmq+fgjAJ66yX0KIq8zl/s0+6u6TzddTaHR0FUJcw1zx12Xd3c2M/tFkZvcBuA8A\nslleQ10Isbxc7pN92szGAKD5P+264O4PuPtOd9+ZybT0q/hCiEu43GB/BMBnm68/C+CnV8cdIcRy\nsRTp7fsAPghg2MxOAPgygK8C+KGZfQ7AUQD3LmVnqVQGnd2DQVs2ouLMzIQ/OLQNcolkoco1niLv\n1oSOgR5qa6sb2SCX3jxyhIsVnuXV3sEnpiLtmuqp8LzuIS795JzLjekOntnmOa591i383qzGpbxU\nmr/nbFeO2jq6ua1aCsus505O0zlDXbwN1T0fu4vadr90hNrmIsUoi6UzwfESafEEAP094Ws/k+bn\nZNFgd/dPE9OHFpsrhLh20DfohEgICnYhEoKCXYiEoGAXIiEo2IVICC39lksu14ax9eFsI0vx+06x\nGM7wmc5z93P9PMurUuVSjUW+5VeYC2dQVZz7nsnwwpHVNLd19vIMsJGhGWrz82G5phzpUWZ17n9H\nRwe1pSJZh3UP769W4zJlKhsp9pnmPs7N8yxGIwUY2yLXW/4Ml+U6OsPSMQC8//Ybqe21N45S295X\npoLjc3mejZgjhUzr9VgGoBAiESjYhUgICnYhEoKCXYiEoGAXIiEo2IVICC2V3twAt7C8UolIQwuz\nYWmlLSILzeYjhSOLvNDjQp7LOFmS9NbTxSW0VQNcqukd5Blgq/r5e6tl+qit0BY+juc38Ky3Um2S\n2hDJzKtVI9l3JEOwluLZiBaR3voHefZdvRbxkVxXfX38+OZ4LRbMzEZkz0pYmgWAm7evprb+nvD1\n8+ijvLjlmelw4dZqJI70ZBciISjYhUgICnYhEoKCXYiEoGAXIiG0ttyrO0BWcDN1vrLbF/7OP8b7\nyPI4gHdt4vXputv5Smza+P1vPh9eiS0uXKRzOroq1LZtC1+pH9+wjtpS2Q3UNjcT9nF8bIz7cZgW\nB0bvIDn4AAYHeLJOJhNONorkacAjiTXtXZ3UVi1GVqDJ/rKxxCtwtWZouJva5ha4KjA/E052AYC1\nq8I17z7xLz9C5/z1z/4uOJ7J8IOoJ7sQCUHBLkRCULALkRAU7EIkBAW7EAlBwS5EQlhK+6cHAXwc\nwGl339Ec+wqAPwbwZt+aL7n7zxfbVk9XJz5w+3uCtk3X30TnnTp5Mji+dg2XrrZu2Uxtq1eNUFva\nuZw3S5IgSpFkEUvx7XV38USY7m4ueaVzXDrMEgmzMB9uMQQAt+7gUt7E1glqq9S5rOjkOVKtc5nM\n0/xYpbP8Uq0UuZ5XJ4khqQx/zlk79wOReaUKPx6ZNK9tWCuHr6tVEZnvzn/63uD4b599mc5ZypP9\nLwDcHRj/hrvf3Py3aKALIVaWRYPd3Z8EwPNFhRC/F1zJ3+xfMLM9ZvagmfFkYyHENcHlBvu3AGwG\ncDOASQBfY79oZveZ2W4z2z03z5P7hRDLy2UFu7tPu3vN3esAvg1gV+R3H3D3ne6+s7uLLzgIIZaX\nywp2M7s0q+KTAPZeHXeEEMvFUqS37wP4IIBhMzsB4MsAPmhmNwNwAEcA/MlSdtbZ2YH33PiuoO3d\nt3DprbAjLKN19fGsK17pDHDj0koqIpEMdoXriEW6P0XvpnXSmgiI1xJDROIplcLtnzZft57O6chx\nCbAwzzP6PBW5fCxs80h9t7pzWy1yzmItj8qF8PGo1fl7TmUi10fkjM6e4xLs0cPHqe2OO28Jji9U\neD3ETiIPRpTexYPd3T8dGP7OYvOEENcW+gadEAlBwS5EQlCwC5EQFOxCJAQFuxAJoaUFJ1OpFDpI\npld3O2+h1NVJ3IwU14sVNrSY9BaTeDwsldUrXEKLyUkWKXpYjYiHMXnFScHM7n6eIVit8X3V6pEq\nkKTFEwA4asHxVMz5GrfVMlwSdURONilwavWwfwDQFnnP2Ro/Z11FPs+nwxIgAJw5NB0cX7eNFx09\nmwp/GzV2ePVkFyIhKNiFSAgKdiESgoJdiISgYBciISjYhUgILZXe0uk0evrCEpBHss0WSmH5xEu8\nJ1eJzAGA+bl5aitX+LxSKZxtVq1y6aoSyVCrRPa1EOkbtjDPs6GqJJOuZ7CPzunp433x+nuGqa09\nF+7nBgA11rvPIn3ZwG09PbwA57nT/DgWC2GJql7nxZUM/H3Va/ya6+3h8vGG9aPUVlgIX48eKc7Z\n1xOWsNMROVdPdiESgoJdiISgYBciISjYhUgICnYhEkJLV+NnZvL460f+JmirZX9N5124EE4UmLt4\nls5JRXIjYiv109PhfQFAjWTXDEbaSQ0MD1FbW5of/vnz4ZZAAHDg9f3Ulp8Lrz6Pb+QtntJZroT0\n9nD/N27kde3WjYfr9W3ctJbOGWzjWRw97dzHeqQWIdLh5JRKja90pyMtntIRH0cnIspFL1+pr3g4\nKSfNRQEMDobfcyaSHKYnuxAJQcEuREJQsAuREBTsQiQEBbsQCUHBLkRCWEr7p3EA3wUwika7pwfc\n/ZtmNgjgBwAm0GgBda+7X4htKz87h8eeeCpo61+3jc7zWlhOeuGpJ+icDet4/a7hIS4nnTwxRW1V\nUresc5AnkpRTPElm+gRvCfShXbdT2803vpvaFkrF4Hgqy0/14WNHqe3A629Q28t7X6C2/r5wE88/\n/KNP0jl3vHsrteUiPbbWjY1TW5lIbxYp1harG1ghtfUAIJWJ1LXr54k8HSR5pZ7mEjETIiMlFJf0\nZK8C+FN3vx7AbQA+b2bXA7gfwOPuvgXA482fhRDXKIsGu7tPuvvzzdezAPYDWAvgHgAPNX/tIQCf\nWC4nhRBXzjv6m93MJgDcAuAZAKPuPtk0TaHxMV8IcY2y5GA3s24APwLwRXfPX2pzdwfCxbvN7D4z\n221mu8tlnvgvhFhelhTsZpZFI9C/5+4/bg5Pm9lY0z4G4HRorrs/4O473X1nLse/HyyEWF4WDXZr\ntE/5DoD97v71S0yPAPhs8/VnAfz06rsnhLhaLCXr7Q4AnwHwspm92Bz7EoCvAvihmX0OwFEA9y62\noYHBIfyrT//roK1tZAudtzAblsNef/klOmdsNZdjUpE6XR3tPIOqXA+38Nm6g/s+MMYz4haGeR20\nj3/0n1NbZ08Htc0T6S3SqQlV0tYKAIrV8PYA4PTp89R29PCp4HhnJz++UyfOUduRfa9TW6rIfTw0\nFfzAiV0f2UnnbJhYQ22xbLlUeyRNLctlOWO15ozPyVn4nMWkt0WD3d1/A4Bt4kOLzRdCXBvoG3RC\nJAQFuxAJQcEuREJQsAuREBTsQiSElhacNAPacuH7y4FX99J5+Yth6c1j2UllnjE0F2n/ZBHtor0t\nnGtUWeDtmC6e4T5OH+NZb3/zt+HCnABwYTayv7mLwfGeXi559Q2EW3IBQFekUOKJE2F5DQBGhsOF\nJdt7uRT565/x93z+9T3UVivzFlsHp8IFRE9EWmht2c6l1L7eTm4b4C22Ojp51ltfV/i6yrbz4pGd\nneHz4s6vXz3ZhUgICnYhEoKCXYiEoGAXIiEo2IVICAp2IRJCS6W3erWC2XNhGe2XP/0ZnXd86kRw\nPFUJZ6EBwJ49eWqLpQZVqzyrCSTT6LFHf0mn5LJcurr5lluprZzrobZ8aYHaDh0LZ3mdO8f7w5WL\nPOvt1NQRajt8hG9z5y3vCY7/28//ezrn2ad/S23VizwjLl/iRVEK4ZoqOLSby56/fm6S2royXObL\n5rhUlm7j10EPkd7WbZigc+75w08Fx8tV/vzWk12IhKBgFyIhKNiFSAgKdiESgoJdiITQ0tX4bDaH\nsdGxoG3LxEY6zxFeLc5EWiulIyvuqTS/x3mdJ67k2rvChixPclizJpwQAgAfvOsuauvpjCRctPPa\nda/sDdflO3CQt3FavXaC2oqRtkvpDu7j3gOvBsdfOXCAzumc2E5tp07x9zzQz20juXBduM5uXsfv\n/BRvh3Xu5EFqO3M2nHQDAMVaJGmLFAicnOHh+b4PhedUedk6PdmFSAoKdiESgoJdiISgYBciISjY\nhUgICnYhEsKi0puZjQP4LhotmR3AA+7+TTP7CoA/BnCm+atfcvefx7ZVrVZx/ky4ZdBt/+R9dN77\nPvCB4HhbG088yETktVj7p3qkFVIa4f1VylzvKJR50sq5E4ep7XyRJ1ycP8vbLh0iEtup0+EEJADo\nHuHtjtDGZUXLcemtXA0npzz2q9/QORs230Bt44NcwmxP8cu4kyQilYq8Bt2h/D5q6+7htfxqzpOo\npi7MUdvw8ERwfKHCr8Vf/urZ4PjsLK+vuBSdvQrgT939eTPrAfCcmT3WtH3D3f/rErYhhFhhltLr\nbRLAZPP1rJntB8Bvs0KIa5J39De7mU0AuAXAM82hL5jZHjN70Mz415iEECvOkoPdzLoB/AjAF909\nD+BbADYDuBmNJ//XyLz7zGy3me2eneN/JwkhlpclBbuZZdEI9O+5+48BwN2n3b3m7nUA3wawKzTX\n3R9w953uvrOnm1dfEUIsL4sGuzVapHwHwH53//ol45dmtHwSAG/pIoRYcZayGn8HgM8AeNnMXmyO\nfQnAp83sZjTkuCMA/mSxDaVShi7StuZcvkjnvbDnueD4yAhfJhgdGaa2SoXLWhcuzFAbimEfM3W+\nvbUbuaw1PsA/6Zw8wOugzc/xmmsjo6uD451D/XROup3LSQsFfl7GxtZT29SpcN3As+fC7akAYGxN\npC1XpNXXXIkff2TC11ulzuXStg6S3QigLZJNWT53htqQCteZA4BRknVYLvEWZuxw8KO0tNX43wAI\nvcOopi6EuLbQN+iESAgKdiESgoJdiISgYBciISjYhUgILS04mTKgLRvO5CkVueT11FOPB8e9wmWh\n3k5eULBS4dlJxQJvKZUh98YNE+N0zo7brqe2zeu5LDdzPCxdAcDUhbPUlusIS02bh8KSHACcOcMz\nsm7YtoPa3n3DNmp7+H99NzieQbgAJABU5vn5LJe5zWNVFtvD5zrWjmli4yZqO338Nb6vFM/C7Oji\n+9u+fWtwvLjAz8v42Ehw/Fc5LvHpyS5EQlCwC5EQFOxCJAQFuxAJQcEuREJQsAuREFoqvdXrdSwU\nSAHGSBHIuz768fD2yjxLKh2R1+o1XsjP01w+SWfCslF7Fy+8ODXDpbzZGd737HyB+2/tvAjkay8e\nCo6f+y3PyNq0kUto771uC7WVIxlxHbmw1OSRjMNYhl0qzS9V0ioNAFCokz6BNX58N6zj0ltx7hy1\nXd/Ls+Wefe4Fajt1NCznFeb59e0LF4Lj5RLPiNSTXYiEoGAXIiEo2IVICAp2IRKCgl2IhKBgFyIh\ntDbrLWXo6g7LV32RSnk9q8JZQaWIzNAeuY/ljGdeeQfPlmvrDM+rF3l20uxsntrSnbzQ48hmXiBy\ncyfPenv9cLjXG4xLillSBBQATk4eo7ahYV7wk9nKBS4nlUq8GOV8JCOuFMkOq5TCUm+mnculo2tW\nUdvRyWlqmz5Gjj2A4hx/b2/sezE4PjTE/fCBwfB4pDCnnuxCJAQFuxAJQcEuREJQsAuREBTsQiSE\nRVfjzawdwJMA2pq//1fu/mUz2wjgYQBDAJ4D8Bl35/1qANTrRSzMkuSPOr/vZK07OD49zVc4X3/l\nCLW1Z/iKe66Pr4IPk3ZTa4b76JxMJMFnqG+I2iK5OigWwkkQADAyEl7hX7smvHoLAJNTU9R24MB+\napsob6Q2ppTMzvJztrDAV7rzF7mqEVuNr5XDiUjpNp60sm8vbx0Wa8k0MjJKbWtv5LX8RlaF5w2v\n4nUD24n/j//DE3TOUp7sJQB/4O43odGe+W4zuw3AnwP4hrtfB+ACgM8tYVtCiBVi0WD3Bm/eOrPN\nfw7gDwD8VXP8IQCfWBYPhRBXhaX2Z083O7ieBvAYgDcAzLj7m0nBJwCsXR4XhRBXgyUFu7vX3P1m\nAOsA7ALwrqXuwMzuM7PdZrZ7dpYUrhBCLDvvaDXe3WcAPAHgdgD9ZvbmAt86ACfJnAfcfae77+zp\n4V9RFEIsL4sGu5mtMrP+5usOAB8GsB+NoP+j5q99FsBPl8tJIcSVs5REmDEAD5lZGo2bww/d/VEz\newXAw2b2nwG8AOA7i26p7qiTNj6pyH0nUwkncfSSVlIA8NzTv6K2qWmeSGJZnhSya9d7guN33r6T\nzrl4kUtNe55/htrmizzx48Cx49R26MiR4Hhhgf8J5c6LuLX38mSMfH6W2mZJi6r5PJcNI6XkkElz\na1/kE+OajWF5cGBojM4ZWcMlrzW33EBtg5EadLlYbUNmiyQvwcPxkoq0oFo02N19D4BbAuOH0Pj7\nXQjxe4C+QSdEQlCwC5EQFOxCJAQFuxAJQcEuREKwWM2qq74zszMAjjZ/HAbANbDWIT/eivx4K79v\nfmxw96Be2tJgf8uOzXa7Oxeo5Yf8kB9X1Q99jBciISjYhUgIKxnsD6zgvi9FfrwV+fFW/tH4sWJ/\nswshWos+xguREFYk2M3sbjN7zcwOmtn9K+FD048jZvaymb1oZrtbuN8Hzey0me29ZGzQzB4zs9eb\n//PeSsvrx1fM7GTzmLxoZh9rgR/jZvaEmb1iZvvM7N81x1t6TCJ+tPSYmFm7mT1rZi81/fhPzfGN\nZvZMM25+YBbpYxbC3Vv6D0AajbJWmwDkALwE4PpW+9H05QiA4RXY7/sB3Apg7yVj/wXA/c3X9wP4\n8xXy4ysA/kOLj8cYgFubr3sAHABwfauPScSPlh4TNLJ9u5uvswCeAXAbgB8C+FRz/H8A+DfvZLsr\n8WTfBeCgux/yRunphwHcswJ+rBju/iSA828bvgeNwp1Aiwp4Ej9ajrtPuvvzzdezaBRHWYsWH5OI\nHy3FG1z1Iq8rEexrAVxafWEli1U6gF+Y2XNmdt8K+fAmo+4+2Xw9BYAXIV9+vmBme5of85f9z4lL\nMbMJNOonPIMVPCZv8wNo8TFZjiKvSV+gu9PdbwXwUQCfN7P3r7RDQOPOjsaNaCX4FoDNaPQImATw\ntVbt2My6AfwIwBfd/S1dIVp5TAJ+tPyY+BUUeWWsRLCfBDB+yc+0WOVy4+4nm/+fBvATrGzlnWkz\nGwOA5v+nV8IJd59uXmh1AN9Gi46JmWXRCLDvufuPm8MtPyYhP1bqmDT3/Y6LvDJWIth/B2BLc2Ux\nB+BTAB5ptRNm1mVmPW++BvARAHvjs5aVR9Ao3AmsYAHPN4OrySfRgmNiZoZGDcP97v71S0wtPSbM\nj1Yfk2Ur8tqqFca3rTZ+DI2VzjcA/NkK+bAJDSXgJQD7WukHgO+j8XGwgsbfXp9Do2fe4wBeB/B3\nAAZXyI+/BPAygD1oBNtYC/y4E42P6HsAvNj897FWH5OIHy09JgBuRKOI6x40biz/8ZJr9lkABwH8\nHwBt72S7+gadEAkh6Qt0QiQGBbsQCUHBLkRCULALkRAU7EIkBAW7EAlBwS5EQlCwC5EQ/h+CqIkl\nWmKmUgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xF_KVNDsoC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_block(X, num_filters, strides):\n",
        "    \n",
        "    x1 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "    x1 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "\n",
        "    x2 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "    x2 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x2)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "\n",
        "    x = Add()([x1,x2])\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8XEgd_Z6zb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(X, num_filters):\n",
        "\n",
        "    x1 = Conv2D(num_filters, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "\n",
        "    x2 = Conv2D(num_filters, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "\n",
        "    x3 = Conv2D(num_filters, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x3 = BatchNormalization()(x3)\n",
        "    x = Add()([x1, x2, x3])\n",
        "    x = Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7wO5JUbOzqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch >= 90:\n",
        "        lr *= 1e-3\n",
        "    elif epoch >= 70:\n",
        "        lr *= 1e-2\n",
        "    elif epoch >= 30:\n",
        "        lr *= 1e-1\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gjfq5i1gwqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(x):\n",
        "    input_img = tf.keras.Input(shape=x.shape[1:])\n",
        "\n",
        "    x = Conv2D(32, kernel_size=(7,7), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(input_img)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='valid')(x)\n",
        "\n",
        "    x = conv_block(x, 64, (1,1))\n",
        "    x = identity_block(x, 64)\n",
        "\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    x = conv_block(x, 128, (1,1))\n",
        "\n",
        "    x = identity_block(x, 128)\n",
        "\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = conv_block(x, 256, (1,1))\n",
        "\n",
        "    x = identity_block(x, 256)\n",
        "\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = AveragePooling2D(pool_size=(4,4), padding='valid')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    y_pred = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[input_img], outputs=[y_pred])\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "                  optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy']\n",
        "                 )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSpfpmj_gg6U",
        "colab_type": "code",
        "outputId": "dc6dedad-028f-47ad-8e57-d78b6cd85b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cifar = CIFAR10()\n",
        "#cifar.data_augmentation(size=10000)\n",
        "cifar.normalize_data()\n",
        "x_train, y_train = cifar.get_train_set()\n",
        "x_test, y_test = cifar.get_test_set()\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "model = create_model(x_train)\n",
        "\n",
        "EPOCHS = 70\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "training = model.fit(\n",
        "                     x_train,\n",
        "                     y_train,\n",
        "                     batch_size=BATCH_SIZE, \n",
        "                     epochs=EPOCHS,\n",
        "                     validation_data=(x_test, y_test),\n",
        "                     callbacks=[lr_scheduler]\n",
        "                    ) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "391/391 [==============================] - 35s 88ms/step - loss: 1.9896 - accuracy: 0.5132 - val_loss: 2.0921 - val_accuracy: 0.4913 - lr: 0.0010\n",
            "Epoch 2/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 1.4486 - accuracy: 0.6659 - val_loss: 1.5618 - val_accuracy: 0.6219 - lr: 0.0010\n",
            "Epoch 3/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 1.1907 - accuracy: 0.7239 - val_loss: 1.3456 - val_accuracy: 0.6558 - lr: 0.0010\n",
            "Epoch 4/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 1.0310 - accuracy: 0.7596 - val_loss: 1.1747 - val_accuracy: 0.7112 - lr: 0.0010\n",
            "Epoch 5/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.9239 - accuracy: 0.7812 - val_loss: 1.0646 - val_accuracy: 0.7355 - lr: 0.0010\n",
            "Epoch 6/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.8584 - accuracy: 0.7962 - val_loss: 1.2584 - val_accuracy: 0.6646 - lr: 0.0010\n",
            "Epoch 7/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.8052 - accuracy: 0.8104 - val_loss: 1.0321 - val_accuracy: 0.7319 - lr: 0.0010\n",
            "Epoch 8/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.7539 - accuracy: 0.8279 - val_loss: 0.9854 - val_accuracy: 0.7451 - lr: 0.0010\n",
            "Epoch 9/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.7247 - accuracy: 0.8361 - val_loss: 1.0184 - val_accuracy: 0.7540 - lr: 0.0010\n",
            "Epoch 10/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.6958 - accuracy: 0.8459 - val_loss: 0.9306 - val_accuracy: 0.7776 - lr: 0.0010\n",
            "Epoch 11/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.6783 - accuracy: 0.8510 - val_loss: 1.1096 - val_accuracy: 0.7247 - lr: 0.0010\n",
            "Epoch 12/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.6604 - accuracy: 0.8588 - val_loss: 1.4818 - val_accuracy: 0.6706 - lr: 0.0010\n",
            "Epoch 13/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.6372 - accuracy: 0.8687 - val_loss: 0.8996 - val_accuracy: 0.7899 - lr: 0.0010\n",
            "Epoch 14/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.6127 - accuracy: 0.8776 - val_loss: 0.9964 - val_accuracy: 0.7778 - lr: 0.0010\n",
            "Epoch 15/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.5934 - accuracy: 0.8847 - val_loss: 1.0701 - val_accuracy: 0.7418 - lr: 0.0010\n",
            "Epoch 16/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.5894 - accuracy: 0.8867 - val_loss: 1.0269 - val_accuracy: 0.7685 - lr: 0.0010\n",
            "Epoch 17/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.5677 - accuracy: 0.8932 - val_loss: 1.3559 - val_accuracy: 0.7253 - lr: 0.0010\n",
            "Epoch 18/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.5588 - accuracy: 0.8980 - val_loss: 1.1179 - val_accuracy: 0.7390 - lr: 0.0010\n",
            "Epoch 19/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.5462 - accuracy: 0.9032 - val_loss: 0.9415 - val_accuracy: 0.7971 - lr: 0.0010\n",
            "Epoch 20/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.5285 - accuracy: 0.9085 - val_loss: 0.9542 - val_accuracy: 0.8002 - lr: 0.0010\n",
            "Epoch 21/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.5242 - accuracy: 0.9110 - val_loss: 1.0694 - val_accuracy: 0.7735 - lr: 0.0010\n",
            "Epoch 22/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.5044 - accuracy: 0.9178 - val_loss: 1.0413 - val_accuracy: 0.7837 - lr: 0.0010\n",
            "Epoch 23/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.4964 - accuracy: 0.9205 - val_loss: 1.1103 - val_accuracy: 0.7753 - lr: 0.0010\n",
            "Epoch 24/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.4903 - accuracy: 0.9219 - val_loss: 1.2350 - val_accuracy: 0.7425 - lr: 0.0010\n",
            "Epoch 25/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.4805 - accuracy: 0.9265 - val_loss: 1.1866 - val_accuracy: 0.7578 - lr: 0.0010\n",
            "Epoch 26/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.4735 - accuracy: 0.9287 - val_loss: 1.6259 - val_accuracy: 0.6947 - lr: 0.0010\n",
            "Epoch 27/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.4714 - accuracy: 0.9294 - val_loss: 1.0784 - val_accuracy: 0.7938 - lr: 0.0010\n",
            "Epoch 28/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.4600 - accuracy: 0.9334 - val_loss: 1.0480 - val_accuracy: 0.7788 - lr: 0.0010\n",
            "Epoch 29/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.4430 - accuracy: 0.9394 - val_loss: 1.3010 - val_accuracy: 0.7752 - lr: 0.0010\n",
            "Epoch 30/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.4501 - accuracy: 0.9359 - val_loss: 1.1952 - val_accuracy: 0.7647 - lr: 0.0010\n",
            "Epoch 31/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.3355 - accuracy: 0.9782 - val_loss: 0.8402 - val_accuracy: 0.8528 - lr: 1.0000e-04\n",
            "Epoch 32/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2899 - accuracy: 0.9913 - val_loss: 0.9625 - val_accuracy: 0.8557 - lr: 1.0000e-04\n",
            "Epoch 33/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2711 - accuracy: 0.9950 - val_loss: 1.0353 - val_accuracy: 0.8574 - lr: 1.0000e-04\n",
            "Epoch 34/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2598 - accuracy: 0.9961 - val_loss: 1.1206 - val_accuracy: 0.8569 - lr: 1.0000e-04\n",
            "Epoch 35/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2500 - accuracy: 0.9972 - val_loss: 1.1380 - val_accuracy: 0.8549 - lr: 1.0000e-04\n",
            "Epoch 36/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2419 - accuracy: 0.9975 - val_loss: 1.1642 - val_accuracy: 0.8558 - lr: 1.0000e-04\n",
            "Epoch 37/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2337 - accuracy: 0.9978 - val_loss: 1.1915 - val_accuracy: 0.8522 - lr: 1.0000e-04\n",
            "Epoch 38/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2282 - accuracy: 0.9977 - val_loss: 1.2535 - val_accuracy: 0.8542 - lr: 1.0000e-04\n",
            "Epoch 39/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2233 - accuracy: 0.9970 - val_loss: 1.2772 - val_accuracy: 0.8501 - lr: 1.0000e-04\n",
            "Epoch 40/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2163 - accuracy: 0.9977 - val_loss: 1.2940 - val_accuracy: 0.8495 - lr: 1.0000e-04\n",
            "Epoch 41/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2108 - accuracy: 0.9977 - val_loss: 1.2472 - val_accuracy: 0.8569 - lr: 1.0000e-04\n",
            "Epoch 42/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2042 - accuracy: 0.9982 - val_loss: 1.2771 - val_accuracy: 0.8517 - lr: 1.0000e-04\n",
            "Epoch 43/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.2003 - accuracy: 0.9976 - val_loss: 1.2877 - val_accuracy: 0.8536 - lr: 1.0000e-04\n",
            "Epoch 44/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.1954 - accuracy: 0.9979 - val_loss: 1.2931 - val_accuracy: 0.8538 - lr: 1.0000e-04\n",
            "Epoch 45/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1925 - accuracy: 0.9974 - val_loss: 1.3915 - val_accuracy: 0.8471 - lr: 1.0000e-04\n",
            "Epoch 46/70\n",
            "391/391 [==============================] - 33s 84ms/step - loss: 0.1903 - accuracy: 0.9968 - val_loss: 1.2678 - val_accuracy: 0.8503 - lr: 1.0000e-04\n",
            "Epoch 47/70\n",
            "391/391 [==============================] - 33s 84ms/step - loss: 0.1846 - accuracy: 0.9979 - val_loss: 1.3430 - val_accuracy: 0.8472 - lr: 1.0000e-04\n",
            "Epoch 48/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1811 - accuracy: 0.9978 - val_loss: 1.2884 - val_accuracy: 0.8476 - lr: 1.0000e-04\n",
            "Epoch 49/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.1782 - accuracy: 0.9975 - val_loss: 1.2706 - val_accuracy: 0.8539 - lr: 1.0000e-04\n",
            "Epoch 50/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.1771 - accuracy: 0.9973 - val_loss: 1.2506 - val_accuracy: 0.8527 - lr: 1.0000e-04\n",
            "Epoch 51/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.1729 - accuracy: 0.9978 - val_loss: 1.2731 - val_accuracy: 0.8496 - lr: 1.0000e-04\n",
            "Epoch 52/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.1682 - accuracy: 0.9984 - val_loss: 1.2064 - val_accuracy: 0.8566 - lr: 1.0000e-04\n",
            "Epoch 53/70\n",
            "391/391 [==============================] - 34s 86ms/step - loss: 0.1673 - accuracy: 0.9977 - val_loss: 1.3201 - val_accuracy: 0.8494 - lr: 1.0000e-04\n",
            "Epoch 54/70\n",
            "391/391 [==============================] - 34s 88ms/step - loss: 0.1672 - accuracy: 0.9968 - val_loss: 1.3381 - val_accuracy: 0.8511 - lr: 1.0000e-04\n",
            "Epoch 55/70\n",
            "391/391 [==============================] - 34s 87ms/step - loss: 0.1634 - accuracy: 0.9977 - val_loss: 1.3980 - val_accuracy: 0.8455 - lr: 1.0000e-04\n",
            "Epoch 56/70\n",
            "391/391 [==============================] - 34s 87ms/step - loss: 0.1592 - accuracy: 0.9981 - val_loss: 1.2763 - val_accuracy: 0.8550 - lr: 1.0000e-04\n",
            "Epoch 57/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1568 - accuracy: 0.9981 - val_loss: 1.3314 - val_accuracy: 0.8507 - lr: 1.0000e-04\n",
            "Epoch 58/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1545 - accuracy: 0.9980 - val_loss: 1.3337 - val_accuracy: 0.8477 - lr: 1.0000e-04\n",
            "Epoch 59/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1555 - accuracy: 0.9971 - val_loss: 1.2984 - val_accuracy: 0.8524 - lr: 1.0000e-04\n",
            "Epoch 60/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1537 - accuracy: 0.9968 - val_loss: 1.2981 - val_accuracy: 0.8501 - lr: 1.0000e-04\n",
            "Epoch 61/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1505 - accuracy: 0.9978 - val_loss: 1.3461 - val_accuracy: 0.8517 - lr: 1.0000e-04\n",
            "Epoch 62/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1492 - accuracy: 0.9975 - val_loss: 1.2817 - val_accuracy: 0.8507 - lr: 1.0000e-04\n",
            "Epoch 63/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1439 - accuracy: 0.9989 - val_loss: 1.3108 - val_accuracy: 0.8573 - lr: 1.0000e-04\n",
            "Epoch 64/70\n",
            "391/391 [==============================] - 33s 84ms/step - loss: 0.1425 - accuracy: 0.9984 - val_loss: 1.2827 - val_accuracy: 0.8542 - lr: 1.0000e-04\n",
            "Epoch 65/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1439 - accuracy: 0.9973 - val_loss: 1.3106 - val_accuracy: 0.8493 - lr: 1.0000e-04\n",
            "Epoch 66/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1439 - accuracy: 0.9969 - val_loss: 1.3440 - val_accuracy: 0.8481 - lr: 1.0000e-04\n",
            "Epoch 67/70\n",
            "391/391 [==============================] - 33s 84ms/step - loss: 0.1404 - accuracy: 0.9977 - val_loss: 1.3231 - val_accuracy: 0.8485 - lr: 1.0000e-04\n",
            "Epoch 68/70\n",
            "391/391 [==============================] - 33s 84ms/step - loss: 0.1389 - accuracy: 0.9980 - val_loss: 1.4104 - val_accuracy: 0.8438 - lr: 1.0000e-04\n",
            "Epoch 69/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1377 - accuracy: 0.9977 - val_loss: 1.3658 - val_accuracy: 0.8485 - lr: 1.0000e-04\n",
            "Epoch 70/70\n",
            "391/391 [==============================] - 33s 85ms/step - loss: 0.1361 - accuracy: 0.9976 - val_loss: 1.2650 - val_accuracy: 0.8469 - lr: 1.0000e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LhOwxPAglrH",
        "colab_type": "code",
        "outputId": "71c8e989-4a1d-45a8-9c4c-f81c0db76de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 32)   4736        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 32)   128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 15, 15, 32)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 15, 15, 64)   18496       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 15, 15, 64)   18496       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 15, 15, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 15, 15, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 15, 15, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 15, 15, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 15, 15, 64)   36928       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 15, 15, 64)   36928       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 15, 15, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 15, 15, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 15, 15, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 15, 15, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 15, 15, 64)   0           activation_2[0][0]               \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 15, 15, 64)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 15, 15, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 15, 15, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 15, 15, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 15, 15, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 15, 15, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 15, 15, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 15, 15, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 15, 15, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 15, 15, 64)   0           activation_6[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 15, 15, 64)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 15, 15, 64)   0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 15, 15, 128)  73856       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 15, 15, 128)  73856       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 15, 15, 128)  512         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 15, 15, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 15, 15, 128)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 15, 15, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 15, 15, 128)  147584      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 15, 15, 128)  147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 15, 15, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 15, 15, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 15, 15, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 15, 15, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 15, 15, 128)  0           activation_10[0][0]              \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 15, 15, 128)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 15, 15, 128)  147584      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 15, 15, 128)  147584      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 15, 15, 128)  512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 15, 15, 128)  512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 15, 15, 128)  147584      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 15, 15, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 15, 15, 128)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 15, 15, 128)  512         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 15, 15, 128)  0           activation_14[0][0]              \n",
            "                                                                 activation_15[0][0]              \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 15, 15, 128)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 15, 15, 128)  0           activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 15, 15, 256)  295168      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 15, 15, 256)  295168      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 15, 15, 256)  1024        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 15, 15, 256)  1024        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 15, 15, 256)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 15, 15, 256)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 15, 15, 256)  590080      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 15, 15, 256)  590080      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 15, 15, 256)  1024        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 15, 15, 256)  1024        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 15, 15, 256)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 15, 15, 256)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 15, 15, 256)  0           activation_18[0][0]              \n",
            "                                                                 activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 15, 15, 256)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 15, 15, 256)  590080      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 15, 15, 256)  590080      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 15, 15, 256)  1024        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 15, 15, 256)  1024        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 15, 15, 256)  590080      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 15, 15, 256)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 15, 15, 256)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 15, 15, 256)  1024        conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 15, 15, 256)  0           activation_22[0][0]              \n",
            "                                                                 activation_23[0][0]              \n",
            "                                                                 batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 15, 15, 256)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 15, 15, 256)  0           activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 3, 3, 256)    0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2304)         0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           23050       flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 4,688,458\n",
            "Trainable params: 4,682,122\n",
            "Non-trainable params: 6,336\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBAKCvO2jnX7",
        "colab_type": "code",
        "outputId": "1be033ec-67f2-4ed4-cec2-9e80097fce31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Test set result after whole training\")\n",
        "validation = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set result after whole training\n",
            "79/79 [==============================] - 2s 26ms/step - loss: 1.2650 - accuracy: 0.8469\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}