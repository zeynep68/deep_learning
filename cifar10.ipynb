{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1URubJ1dyzWyF_ye_G_13Q2RqM2pCgPEP",
      "authorship_tag": "ABX9TyPZ4ZUoteqzxVt4EHs+DdXD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeynep68/deep_learning/blob/master/cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWd-R9LggVRh",
        "colab_type": "code",
        "outputId": "6cee42e5-a4be-40db-df5f-ad2419403e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import Model \n",
        "from tensorflow.keras.optimizers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH5F6Mhygcb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CIFAR10():\n",
        "    def __init__(self):\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "        self.width = self.x_train.shape[1]\n",
        "        self.height = self.x_train.shape[2]\n",
        "        self.depth = self.x_train.shape[3]\n",
        "        \n",
        "        self.num_classes = np.max(self.y_train) + 1\n",
        "        \n",
        "        self.train_size = self.x_train.shape[0]\n",
        "        self.test_size = self.x_test.shape[0]\n",
        "        \n",
        "        self.y_train = to_categorical(self.y_train, self.num_classes)\n",
        "        self.y_test = to_categorical(self.y_test, self.num_classes)\n",
        "\n",
        "        self.scaler = StandardScaler()    \n",
        "\n",
        "    def get_train_set(self):\n",
        "        return self.x_train, self.y_train\n",
        "    \n",
        "    def get_test_set(self):\n",
        "        return self.x_test, self.y_test\n",
        "\n",
        "    def normalize_data(self):\n",
        "        self.scaler.fit(self.x_train.reshape(-1, 1024))\n",
        "    \n",
        "        self.x_train = self.scaler.transform(self.x_train.reshape(-1, 1024))\n",
        "        self.x_test = self.scaler.transform(self.x_test.reshape(-1,1024))\n",
        "\n",
        "        self.x_train = self.x_train.reshape((self.train_size, self.width, self.height, self.depth))\n",
        "        self.x_test = self.x_test.reshape((self.test_size, self.width, self.height, self.depth))\n",
        "        \n",
        "    def data_augmentation(self, size=5000):\n",
        "        img_generator = ImageDataGenerator(\n",
        "                            rotation_range=15,\n",
        "                            zoom_range=0.05,\n",
        "                            width_shift_range=0.1,\n",
        "                            height_shift_range=0.1,\n",
        "                            brightness_range=None,\n",
        "                            horizontal_flip=False,\n",
        "                            vertical_flip=False\n",
        "                            )\n",
        "        img_generator.fit(self.x_train, augment=True) \n",
        "\n",
        "        rand_numbers = np.random.randint(self.train_size, size=size)\n",
        "        x_augmented = np.zeros(shape=(1,self.width, self.height, self.depth))\n",
        "        y_augmented = np.zeros(shape=(1,self.num_classes))\n",
        "\n",
        "        for i in range(size):\n",
        "            x_augmented = np.vstack((x_augmented, self.x_train[ rand_numbers[i] ].reshape(1,self.width, self.height, self.depth)))\n",
        "            y_augmented = np.vstack( (y_augmented, self.y_train[rand_numbers[i]].reshape(1,self.num_classes) ) )\n",
        "            \n",
        "        x_augmented = np.delete(x_augmented, 0, axis=0)\n",
        "        y_augmented = np.delete(y_augmented, 0, axis=0)\n",
        "\n",
        "        x_augmented = img_generator.flow(x_augmented, np.zeros(size), batch_size=size, shuffle=False).next()[0] # next  \n",
        "\n",
        "        self.x_train = np.concatenate((self.x_train, x_augmented))\n",
        "        self.y_train = np.concatenate((self.y_train, y_augmented))\n",
        "\n",
        "        self.train_size = self.x_train.shape[0]\n",
        "\n",
        "        # Ã¤quivalent zur obigen for-schleife: \n",
        "        # x_augmented = self.x_train[rand_numbers].copy()\n",
        "        # y_augmented = self.y_train[rand_numbers].copy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65IxW2hLgfJG",
        "colab_type": "code",
        "outputId": "d38b4c69-e1a9-4719-fc13-c542c7ef6dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    cifar10 = CIFAR10()\n",
        "    img = cifar10.x_train[1]\n",
        "    plt.imshow(img)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWmKmUgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xF_KVNDsoC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_block(X, num_filters, strides):\n",
        "    \n",
        "    x1 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "    x1 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "\n",
        "    x2 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "    x2 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x2)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "\n",
        "    x = Add()([x1,x2])\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8XEgd_Z6zb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(X, num_filters):\n",
        "\n",
        "    x1 = Conv2D(num_filters, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "\n",
        "    x2 = Conv2D(num_filters, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "\n",
        "    x3 = Conv2D(num_filters, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x3 = BatchNormalization()(x3)\n",
        "\n",
        "    x = Add()([x1, x2, x3])\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7wO5JUbOzqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "\n",
        "    if epoch > 35:\n",
        "        lr *= 1e-2\n",
        "    elif epoch >= 30:\n",
        "        lr *= 1e-1\n",
        "\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gjfq5i1gwqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(x):\n",
        "    input_img = tf.keras.Input(shape=x.shape[1:])\n",
        "\n",
        "    x = Conv2D(32, kernel_size=(7,7), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(input_img)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='valid')(x)\n",
        "\n",
        "    x = conv_block(x, 64, (1,1))\n",
        "    x = identity_block(x, 64)\n",
        "\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    x = conv_block(x, 128, (1,1))\n",
        "\n",
        "    x = identity_block(x, 128)\n",
        "\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = conv_block(x, 256, (2,2))\n",
        "\n",
        "    x = identity_block(x, 256)\n",
        "\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = AveragePooling2D(pool_size=(4,4), padding='valid')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    y_pred = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[input_img], outputs=[y_pred])\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "                  optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy']\n",
        "                 )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSpfpmj_gg6U",
        "colab_type": "code",
        "outputId": "31389755-c810-499e-b57e-1e69e4dc7ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cifar = CIFAR10()\n",
        "#cifar.data_augmentation(size=5000)\n",
        "cifar.normalize_data()\n",
        "x_train, y_train = cifar.get_train_set()\n",
        "x_test, y_test = cifar.get_test_set()\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "model = create_model(x_train)\n",
        "\n",
        "EPOCHS = 45\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "training = model.fit(\n",
        "                     x_train,\n",
        "                     y_train,\n",
        "                     batch_size=BATCH_SIZE, \n",
        "                     epochs=EPOCHS,\n",
        "                     validation_data=(x_test, y_test),\n",
        "                     callbacks=[lr_scheduler]\n",
        "                    ) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 1.9330 - accuracy: 0.5288 - val_loss: 2.7600 - val_accuracy: 0.4517 - lr: 0.0010\n",
            "Epoch 2/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 1.4525 - accuracy: 0.6773 - val_loss: 2.3372 - val_accuracy: 0.4755 - lr: 0.0010\n",
            "Epoch 3/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 1.2126 - accuracy: 0.7350 - val_loss: 1.5176 - val_accuracy: 0.6291 - lr: 0.0010\n",
            "Epoch 4/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 1.0629 - accuracy: 0.7690 - val_loss: 1.2474 - val_accuracy: 0.7008 - lr: 0.0010\n",
            "Epoch 5/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.9571 - accuracy: 0.7931 - val_loss: 1.1680 - val_accuracy: 0.7323 - lr: 0.0010\n",
            "Epoch 6/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.8922 - accuracy: 0.8090 - val_loss: 1.1226 - val_accuracy: 0.7435 - lr: 0.0010\n",
            "Epoch 7/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.8377 - accuracy: 0.8234 - val_loss: 1.3928 - val_accuracy: 0.6521 - lr: 0.0010\n",
            "Epoch 8/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.8038 - accuracy: 0.8373 - val_loss: 1.0275 - val_accuracy: 0.7590 - lr: 0.0010\n",
            "Epoch 9/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.7684 - accuracy: 0.8448 - val_loss: 1.4662 - val_accuracy: 0.6539 - lr: 0.0010\n",
            "Epoch 10/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.7483 - accuracy: 0.8570 - val_loss: 1.8805 - val_accuracy: 0.6133 - lr: 0.0010\n",
            "Epoch 11/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.7230 - accuracy: 0.8648 - val_loss: 1.1643 - val_accuracy: 0.7398 - lr: 0.0010\n",
            "Epoch 12/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.7028 - accuracy: 0.8706 - val_loss: 1.1782 - val_accuracy: 0.7433 - lr: 0.0010\n",
            "Epoch 13/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.6870 - accuracy: 0.8771 - val_loss: 1.1817 - val_accuracy: 0.7416 - lr: 0.0010\n",
            "Epoch 14/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.6676 - accuracy: 0.8855 - val_loss: 1.2645 - val_accuracy: 0.7315 - lr: 0.0010\n",
            "Epoch 15/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.6580 - accuracy: 0.8912 - val_loss: 1.1710 - val_accuracy: 0.7497 - lr: 0.0010\n",
            "Epoch 16/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.6389 - accuracy: 0.8977 - val_loss: 1.2147 - val_accuracy: 0.7434 - lr: 0.0010\n",
            "Epoch 17/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.6256 - accuracy: 0.9021 - val_loss: 1.4610 - val_accuracy: 0.6900 - lr: 0.0010\n",
            "Epoch 18/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.6194 - accuracy: 0.9076 - val_loss: 1.1795 - val_accuracy: 0.7618 - lr: 0.0010\n",
            "Epoch 19/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.6044 - accuracy: 0.9104 - val_loss: 1.1129 - val_accuracy: 0.7758 - lr: 0.0010\n",
            "Epoch 20/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5946 - accuracy: 0.9150 - val_loss: 1.0614 - val_accuracy: 0.7851 - lr: 0.0010\n",
            "Epoch 21/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5873 - accuracy: 0.9176 - val_loss: 1.9068 - val_accuracy: 0.6368 - lr: 0.0010\n",
            "Epoch 22/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5772 - accuracy: 0.9212 - val_loss: 1.3002 - val_accuracy: 0.7503 - lr: 0.0010\n",
            "Epoch 23/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5719 - accuracy: 0.9246 - val_loss: 1.4625 - val_accuracy: 0.7073 - lr: 0.0010\n",
            "Epoch 24/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5511 - accuracy: 0.9305 - val_loss: 1.1516 - val_accuracy: 0.7932 - lr: 0.0010\n",
            "Epoch 25/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5565 - accuracy: 0.9285 - val_loss: 1.5221 - val_accuracy: 0.7170 - lr: 0.0010\n",
            "Epoch 26/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5480 - accuracy: 0.9316 - val_loss: 1.4793 - val_accuracy: 0.7211 - lr: 0.0010\n",
            "Epoch 27/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5351 - accuracy: 0.9352 - val_loss: 1.5137 - val_accuracy: 0.7366 - lr: 0.0010\n",
            "Epoch 28/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5256 - accuracy: 0.9395 - val_loss: 1.5384 - val_accuracy: 0.7264 - lr: 0.0010\n",
            "Epoch 29/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5285 - accuracy: 0.9375 - val_loss: 1.2308 - val_accuracy: 0.7699 - lr: 0.0010\n",
            "Epoch 30/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5228 - accuracy: 0.9395 - val_loss: 1.2451 - val_accuracy: 0.7716 - lr: 0.0010\n",
            "Epoch 31/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.4140 - accuracy: 0.9816 - val_loss: 0.8238 - val_accuracy: 0.8586 - lr: 1.0000e-04\n",
            "Epoch 32/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3675 - accuracy: 0.9940 - val_loss: 0.8328 - val_accuracy: 0.8600 - lr: 1.0000e-04\n",
            "Epoch 33/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3446 - accuracy: 0.9971 - val_loss: 0.8430 - val_accuracy: 0.8620 - lr: 1.0000e-04\n",
            "Epoch 34/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3262 - accuracy: 0.9981 - val_loss: 0.8533 - val_accuracy: 0.8627 - lr: 1.0000e-04\n",
            "Epoch 35/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3091 - accuracy: 0.9988 - val_loss: 0.8546 - val_accuracy: 0.8637 - lr: 1.0000e-04\n",
            "Epoch 36/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2923 - accuracy: 0.9991 - val_loss: 0.8670 - val_accuracy: 0.8640 - lr: 1.0000e-04\n",
            "Epoch 37/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2819 - accuracy: 0.9997 - val_loss: 0.8693 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
            "Epoch 38/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2797 - accuracy: 0.9997 - val_loss: 0.8679 - val_accuracy: 0.8642 - lr: 1.0000e-05\n",
            "Epoch 39/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2775 - accuracy: 0.9996 - val_loss: 0.8676 - val_accuracy: 0.8651 - lr: 1.0000e-05\n",
            "Epoch 40/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2750 - accuracy: 0.9997 - val_loss: 0.8693 - val_accuracy: 0.8651 - lr: 1.0000e-05\n",
            "Epoch 41/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2723 - accuracy: 0.9997 - val_loss: 0.8702 - val_accuracy: 0.8644 - lr: 1.0000e-05\n",
            "Epoch 42/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2693 - accuracy: 0.9998 - val_loss: 0.8708 - val_accuracy: 0.8657 - lr: 1.0000e-05\n",
            "Epoch 43/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2661 - accuracy: 0.9999 - val_loss: 0.8698 - val_accuracy: 0.8658 - lr: 1.0000e-05\n",
            "Epoch 44/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2631 - accuracy: 0.9998 - val_loss: 0.8716 - val_accuracy: 0.8651 - lr: 1.0000e-05\n",
            "Epoch 45/45\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.2597 - accuracy: 0.9998 - val_loss: 0.8740 - val_accuracy: 0.8665 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LhOwxPAglrH",
        "colab_type": "code",
        "outputId": "8566a207-7b85-41b3-d782-1dbeedca02c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 32)   4736        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 32)   128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 15, 15, 32)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 15, 15, 64)   18496       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 15, 15, 64)   18496       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 15, 15, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 15, 15, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 15, 15, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 15, 15, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 15, 15, 64)   36928       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 15, 15, 64)   36928       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 15, 15, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 15, 15, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 15, 15, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 15, 15, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 15, 15, 64)   0           activation_2[0][0]               \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 15, 15, 64)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 15, 15, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 15, 15, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 15, 15, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 15, 15, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 15, 15, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 15, 15, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 15, 15, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 15, 15, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 15, 15, 64)   0           activation_6[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 15, 15, 64)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 15, 15, 64)   0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 15, 15, 128)  73856       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 15, 15, 128)  73856       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 15, 15, 128)  512         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 15, 15, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 15, 15, 128)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 15, 15, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 15, 15, 128)  147584      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 15, 15, 128)  147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 15, 15, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 15, 15, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 15, 15, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 15, 15, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 15, 15, 128)  0           activation_10[0][0]              \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 15, 15, 128)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 15, 15, 128)  147584      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 15, 15, 128)  147584      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 15, 15, 128)  512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 15, 15, 128)  512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 15, 15, 128)  147584      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 15, 15, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 15, 15, 128)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 15, 15, 128)  512         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 15, 15, 128)  0           activation_14[0][0]              \n",
            "                                                                 activation_15[0][0]              \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 15, 15, 128)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 15, 15, 128)  0           activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 256)    295168      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 256)    295168      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 256)    1024        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 256)    1024        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 256)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 256)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 256)    590080      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 256)    590080      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 256)    1024        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 4, 4, 256)    1024        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 4, 4, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 4, 4, 256)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 4, 4, 256)    0           activation_18[0][0]              \n",
            "                                                                 activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 4, 4, 256)    0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 256)    590080      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 4, 4, 256)    590080      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 4, 4, 256)    1024        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 4, 4, 256)    1024        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 4, 4, 256)    590080      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 4, 4, 256)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 4, 4, 256)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 4, 4, 256)    1024        conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 4, 4, 256)    0           activation_22[0][0]              \n",
            "                                                                 activation_23[0][0]              \n",
            "                                                                 batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 4, 4, 256)    0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 4, 4, 256)    0           activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 256)          0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 4,667,978\n",
            "Trainable params: 4,661,642\n",
            "Non-trainable params: 6,336\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBAKCvO2jnX7",
        "colab_type": "code",
        "outputId": "9c2a7b30-7b4a-43eb-8577-c3e8401a09af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Test set result after whole training\")\n",
        "validation = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set result after whole training\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.8740 - accuracy: 0.8665\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}