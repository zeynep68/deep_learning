{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1URubJ1dyzWyF_ye_G_13Q2RqM2pCgPEP",
      "authorship_tag": "ABX9TyNexFCPav8l7yS08djTl3xo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeynep68/deep_learning/blob/master/cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWd-R9LggVRh",
        "colab_type": "code",
        "outputId": "5311b26d-da1d-4d54-8809-9656276f0170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import Model \n",
        "from tensorflow.keras.optimizers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH5F6Mhygcb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CIFAR10():\n",
        "    def __init__(self):\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "        self.width = self.x_train.shape[1]\n",
        "        self.height = self.x_train.shape[2]\n",
        "        self.depth = self.x_train.shape[3]\n",
        "        \n",
        "        self.num_classes = np.max(self.y_train) + 1\n",
        "        \n",
        "        self.train_size = self.x_train.shape[0]\n",
        "        self.test_size = self.x_test.shape[0]\n",
        "        \n",
        "        self.y_train = to_categorical(self.y_train, self.num_classes)\n",
        "        self.y_test = to_categorical(self.y_test, self.num_classes)\n",
        "\n",
        "        self.scaler = StandardScaler()    \n",
        "\n",
        "    def get_train_set(self):\n",
        "        return self.x_train, self.y_train\n",
        "    \n",
        "    def get_test_set(self):\n",
        "        return self.x_test, self.y_test\n",
        "\n",
        "    # ZCA preprocessing\n",
        "    def normalize_data(self):\n",
        "        self.scaler.fit(self.x_train.reshape(-1, 1024))\n",
        "    \n",
        "        self.x_train = self.scaler.transform(self.x_train.reshape(-1, 1024))\n",
        "        self.x_test = self.scaler.transform(self.x_test.reshape(-1,1024))\n",
        "\n",
        "        self.x_train = self.x_train.reshape((self.train_size, self.width, self.height, self.depth))\n",
        "        self.x_test = self.x_test.reshape((self.test_size, self.width, self.height, self.depth))\n",
        "\n",
        "    # data augmentation we do horizontal flips and take random crops from image \n",
        "    # padded by 4 pixels on each side, filling missing pixels with reflections of original image. \n",
        "    def data_augmentation(self, X, size=5000):\n",
        "        rand_numbers = np.random.randint(self.train_size, size=size)\n",
        "\n",
        "        x_flipped = tf.image.flip_left_right(X[rand_numbers])\n",
        "        y_flipped = self.y_train[rand_numbers]\n",
        "\n",
        "       #x_augmented = tf.image.random_crop(x_flipped, 4)\n",
        "\n",
        "        self.x_train = np.concatenate((self.x_train, x_flipped))\n",
        "        self.y_train = np.concatenate((self.y_train, y_flipped))\n",
        "\n",
        "        return self.x_train, self.y_train\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xF_KVNDsoC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_block(X, num_filters, strides):\n",
        "    \n",
        "    x1 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "    x1 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "\n",
        "    x2 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "    x2 = Conv2D(num_filters, kernel_size=(3,3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x2)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "\n",
        "    x = Add()([x1,x2])\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8XEgd_Z6zb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(X, num_filters):\n",
        "\n",
        "    num_filters = X.shape[3]\n",
        "\n",
        "    x = Conv2D(num_filters, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(X)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    x = Conv2D(num_filters, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Add()([X, x])\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7wO5JUbOzqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "\n",
        "    if(epoch > 100):\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 75:\n",
        "        lr *= 1e-2\n",
        "    elif epoch >= 40:\n",
        "        lr *= 1e-1\n",
        "\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gjfq5i1gwqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(x):\n",
        "    input_img = tf.keras.Input(shape=x.shape[1:])\n",
        "\n",
        "    x = Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(input_img)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    x = conv_block(x, 64, (1,1))\n",
        "    x = identity_block(x, 64)\n",
        "\n",
        "    x = Dropout(0.15)(x)\n",
        "\n",
        "    x = conv_block(x, 128, (1,1))\n",
        "\n",
        "    x = identity_block(x, 128)\n",
        "\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = conv_block(x, 256, (2,2))\n",
        "\n",
        "    x = identity_block(x, 256)\n",
        "\n",
        "    x = Dropout(0.25)(x)\n",
        "\n",
        "    x = AveragePooling2D(pool_size=(8,8), padding='valid')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    y_pred = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[input_img], outputs=[y_pred])\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "                  optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy']\n",
        "                 )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSpfpmj_gg6U",
        "colab_type": "code",
        "outputId": "137c329a-ddd9-4d81-c290-447594c2b418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cifar = CIFAR10()\n",
        "cifar.normalize_data()\n",
        "\n",
        "x_train, y_train = cifar.get_train_set()\n",
        "x_test, y_test = cifar.get_test_set()\n",
        "\n",
        "#x_train, y_train = cifar.data_augmentation(x_train)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "model = create_model(x_train)\n",
        "\n",
        "EPOCHS = 120\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "training = model.fit(\n",
        "                     x_train,\n",
        "                     y_train,\n",
        "                     batch_size=BATCH_SIZE, \n",
        "                     epochs=EPOCHS,\n",
        "                     validation_data=(x_test, y_test),\n",
        "                     callbacks=[lr_scheduler]\n",
        "                    )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "391/391 [==============================] - 58s 148ms/step - loss: 1.8150 - accuracy: 0.5344 - val_loss: 3.8111 - val_accuracy: 0.2427 - lr: 0.0010\n",
            "Epoch 2/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 1.3055 - accuracy: 0.6929 - val_loss: 1.4590 - val_accuracy: 0.6464 - lr: 0.0010\n",
            "Epoch 3/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 1.0809 - accuracy: 0.7539 - val_loss: 1.6856 - val_accuracy: 0.5832 - lr: 0.0010\n",
            "Epoch 4/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.9289 - accuracy: 0.7941 - val_loss: 1.2646 - val_accuracy: 0.6740 - lr: 0.0010\n",
            "Epoch 5/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.8397 - accuracy: 0.8150 - val_loss: 1.8676 - val_accuracy: 0.5682 - lr: 0.0010\n",
            "Epoch 6/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.7809 - accuracy: 0.8320 - val_loss: 1.7748 - val_accuracy: 0.6479 - lr: 0.0010\n",
            "Epoch 7/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.7363 - accuracy: 0.8447 - val_loss: 1.5169 - val_accuracy: 0.6173 - lr: 0.0010\n",
            "Epoch 8/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.7055 - accuracy: 0.8554 - val_loss: 2.1894 - val_accuracy: 0.5337 - lr: 0.0010\n",
            "Epoch 9/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.6806 - accuracy: 0.8676 - val_loss: 1.3446 - val_accuracy: 0.7071 - lr: 0.0010\n",
            "Epoch 10/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.6567 - accuracy: 0.8746 - val_loss: 1.6121 - val_accuracy: 0.6476 - lr: 0.0010\n",
            "Epoch 11/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.6422 - accuracy: 0.8803 - val_loss: 1.7327 - val_accuracy: 0.6013 - lr: 0.0010\n",
            "Epoch 12/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.6224 - accuracy: 0.8895 - val_loss: 1.9884 - val_accuracy: 0.6133 - lr: 0.0010\n",
            "Epoch 13/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.6145 - accuracy: 0.8935 - val_loss: 1.4171 - val_accuracy: 0.6817 - lr: 0.0010\n",
            "Epoch 14/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.6048 - accuracy: 0.8994 - val_loss: 1.0623 - val_accuracy: 0.7782 - lr: 0.0010\n",
            "Epoch 15/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5889 - accuracy: 0.9062 - val_loss: 0.9056 - val_accuracy: 0.8083 - lr: 0.0010\n",
            "Epoch 16/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5833 - accuracy: 0.9078 - val_loss: 1.0684 - val_accuracy: 0.7976 - lr: 0.0010\n",
            "Epoch 17/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5782 - accuracy: 0.9117 - val_loss: 1.0515 - val_accuracy: 0.7733 - lr: 0.0010\n",
            "Epoch 18/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5688 - accuracy: 0.9160 - val_loss: 1.2788 - val_accuracy: 0.7436 - lr: 0.0010\n",
            "Epoch 19/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5551 - accuracy: 0.9229 - val_loss: 1.2463 - val_accuracy: 0.7407 - lr: 0.0010\n",
            "Epoch 20/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5465 - accuracy: 0.9263 - val_loss: 1.0738 - val_accuracy: 0.7866 - lr: 0.0010\n",
            "Epoch 21/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5470 - accuracy: 0.9267 - val_loss: 1.0134 - val_accuracy: 0.7986 - lr: 0.0010\n",
            "Epoch 22/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5362 - accuracy: 0.9324 - val_loss: 1.2526 - val_accuracy: 0.7584 - lr: 0.0010\n",
            "Epoch 23/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5346 - accuracy: 0.9336 - val_loss: 0.8888 - val_accuracy: 0.8263 - lr: 0.0010\n",
            "Epoch 24/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5279 - accuracy: 0.9354 - val_loss: 1.2833 - val_accuracy: 0.7434 - lr: 0.0010\n",
            "Epoch 25/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5234 - accuracy: 0.9381 - val_loss: 1.0570 - val_accuracy: 0.7819 - lr: 0.0010\n",
            "Epoch 26/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5132 - accuracy: 0.9424 - val_loss: 1.4250 - val_accuracy: 0.7508 - lr: 0.0010\n",
            "Epoch 27/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5146 - accuracy: 0.9427 - val_loss: 1.2664 - val_accuracy: 0.7661 - lr: 0.0010\n",
            "Epoch 28/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5131 - accuracy: 0.9437 - val_loss: 1.2466 - val_accuracy: 0.7767 - lr: 0.0010\n",
            "Epoch 29/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5023 - accuracy: 0.9470 - val_loss: 1.0554 - val_accuracy: 0.8098 - lr: 0.0010\n",
            "Epoch 30/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5004 - accuracy: 0.9491 - val_loss: 1.1162 - val_accuracy: 0.8000 - lr: 0.0010\n",
            "Epoch 31/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.5029 - accuracy: 0.9472 - val_loss: 1.7287 - val_accuracy: 0.6795 - lr: 0.0010\n",
            "Epoch 32/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4942 - accuracy: 0.9513 - val_loss: 1.2831 - val_accuracy: 0.7656 - lr: 0.0010\n",
            "Epoch 33/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4990 - accuracy: 0.9507 - val_loss: 1.1394 - val_accuracy: 0.7761 - lr: 0.0010\n",
            "Epoch 34/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4866 - accuracy: 0.9546 - val_loss: 1.9949 - val_accuracy: 0.6972 - lr: 0.0010\n",
            "Epoch 35/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4853 - accuracy: 0.9540 - val_loss: 1.3790 - val_accuracy: 0.7475 - lr: 0.0010\n",
            "Epoch 36/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4815 - accuracy: 0.9552 - val_loss: 1.0883 - val_accuracy: 0.8072 - lr: 0.0010\n",
            "Epoch 37/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4788 - accuracy: 0.9559 - val_loss: 1.6710 - val_accuracy: 0.7161 - lr: 0.0010\n",
            "Epoch 38/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4823 - accuracy: 0.9558 - val_loss: 1.2027 - val_accuracy: 0.7925 - lr: 0.0010\n",
            "Epoch 39/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4761 - accuracy: 0.9572 - val_loss: 1.4460 - val_accuracy: 0.7711 - lr: 0.0010\n",
            "Epoch 40/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4739 - accuracy: 0.9578 - val_loss: 1.2388 - val_accuracy: 0.7867 - lr: 0.0010\n",
            "Epoch 41/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.4020 - accuracy: 0.9842 - val_loss: 0.6870 - val_accuracy: 0.8993 - lr: 1.0000e-04\n",
            "Epoch 42/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.3660 - accuracy: 0.9941 - val_loss: 0.6765 - val_accuracy: 0.8979 - lr: 1.0000e-04\n",
            "Epoch 43/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.3483 - accuracy: 0.9962 - val_loss: 0.6675 - val_accuracy: 0.9000 - lr: 1.0000e-04\n",
            "Epoch 44/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.3310 - accuracy: 0.9977 - val_loss: 0.6682 - val_accuracy: 0.8997 - lr: 1.0000e-04\n",
            "Epoch 45/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.3152 - accuracy: 0.9983 - val_loss: 0.6456 - val_accuracy: 0.9031 - lr: 1.0000e-04\n",
            "Epoch 46/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.2997 - accuracy: 0.9985 - val_loss: 0.6419 - val_accuracy: 0.9024 - lr: 1.0000e-04\n",
            "Epoch 47/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.2830 - accuracy: 0.9989 - val_loss: 0.6215 - val_accuracy: 0.9038 - lr: 1.0000e-04\n",
            "Epoch 48/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.2675 - accuracy: 0.9989 - val_loss: 0.6192 - val_accuracy: 0.9018 - lr: 1.0000e-04\n",
            "Epoch 49/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.2512 - accuracy: 0.9992 - val_loss: 0.5937 - val_accuracy: 0.9038 - lr: 1.0000e-04\n",
            "Epoch 50/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.2368 - accuracy: 0.9989 - val_loss: 0.6358 - val_accuracy: 0.8980 - lr: 1.0000e-04\n",
            "Epoch 51/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.2233 - accuracy: 0.9987 - val_loss: 0.6062 - val_accuracy: 0.9010 - lr: 1.0000e-04\n",
            "Epoch 52/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.2106 - accuracy: 0.9987 - val_loss: 0.5969 - val_accuracy: 0.9010 - lr: 1.0000e-04\n",
            "Epoch 53/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1993 - accuracy: 0.9987 - val_loss: 0.5916 - val_accuracy: 0.8984 - lr: 1.0000e-04\n",
            "Epoch 54/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1883 - accuracy: 0.9991 - val_loss: 0.5823 - val_accuracy: 0.8998 - lr: 1.0000e-04\n",
            "Epoch 55/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1794 - accuracy: 0.9989 - val_loss: 0.6017 - val_accuracy: 0.8996 - lr: 1.0000e-04\n",
            "Epoch 56/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1714 - accuracy: 0.9988 - val_loss: 0.5944 - val_accuracy: 0.8949 - lr: 1.0000e-04\n",
            "Epoch 57/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1648 - accuracy: 0.9983 - val_loss: 0.6021 - val_accuracy: 0.8931 - lr: 1.0000e-04\n",
            "Epoch 58/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1583 - accuracy: 0.9989 - val_loss: 0.5650 - val_accuracy: 0.8996 - lr: 1.0000e-04\n",
            "Epoch 59/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1521 - accuracy: 0.9987 - val_loss: 0.5945 - val_accuracy: 0.8965 - lr: 1.0000e-04\n",
            "Epoch 60/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1472 - accuracy: 0.9985 - val_loss: 0.6695 - val_accuracy: 0.8813 - lr: 1.0000e-04\n",
            "Epoch 61/120\n",
            "391/391 [==============================] - 58s 147ms/step - loss: 0.1427 - accuracy: 0.9985 - val_loss: 0.6016 - val_accuracy: 0.8945 - lr: 1.0000e-04\n",
            "Epoch 62/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1379 - accuracy: 0.9987 - val_loss: 0.5487 - val_accuracy: 0.9010 - lr: 1.0000e-04\n",
            "Epoch 63/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1328 - accuracy: 0.9988 - val_loss: 0.5686 - val_accuracy: 0.8968 - lr: 1.0000e-04\n",
            "Epoch 64/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1290 - accuracy: 0.9988 - val_loss: 0.5491 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
            "Epoch 65/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1259 - accuracy: 0.9988 - val_loss: 0.6680 - val_accuracy: 0.8796 - lr: 1.0000e-04\n",
            "Epoch 66/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1232 - accuracy: 0.9984 - val_loss: 0.5846 - val_accuracy: 0.8948 - lr: 1.0000e-04\n",
            "Epoch 67/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1207 - accuracy: 0.9982 - val_loss: 0.5578 - val_accuracy: 0.8971 - lr: 1.0000e-04\n",
            "Epoch 68/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1169 - accuracy: 0.9987 - val_loss: 0.5865 - val_accuracy: 0.8921 - lr: 1.0000e-04\n",
            "Epoch 69/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1150 - accuracy: 0.9983 - val_loss: 0.5820 - val_accuracy: 0.8951 - lr: 1.0000e-04\n",
            "Epoch 70/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1108 - accuracy: 0.9990 - val_loss: 0.5607 - val_accuracy: 0.8997 - lr: 1.0000e-04\n",
            "Epoch 71/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1110 - accuracy: 0.9976 - val_loss: 0.6453 - val_accuracy: 0.8827 - lr: 1.0000e-04\n",
            "Epoch 72/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1066 - accuracy: 0.9990 - val_loss: 0.5621 - val_accuracy: 0.8972 - lr: 1.0000e-04\n",
            "Epoch 73/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1063 - accuracy: 0.9983 - val_loss: 0.6100 - val_accuracy: 0.8920 - lr: 1.0000e-04\n",
            "Epoch 74/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1027 - accuracy: 0.9986 - val_loss: 0.5707 - val_accuracy: 0.8961 - lr: 1.0000e-04\n",
            "Epoch 75/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1025 - accuracy: 0.9982 - val_loss: 0.6329 - val_accuracy: 0.8813 - lr: 1.0000e-04\n",
            "Epoch 76/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.1025 - accuracy: 0.9977 - val_loss: 0.6942 - val_accuracy: 0.8778 - lr: 1.0000e-04\n",
            "Epoch 77/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0975 - accuracy: 0.9994 - val_loss: 0.5123 - val_accuracy: 0.9034 - lr: 1.0000e-05\n",
            "Epoch 78/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0962 - accuracy: 0.9995 - val_loss: 0.5088 - val_accuracy: 0.9045 - lr: 1.0000e-05\n",
            "Epoch 79/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0958 - accuracy: 0.9995 - val_loss: 0.5095 - val_accuracy: 0.9043 - lr: 1.0000e-05\n",
            "Epoch 80/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0950 - accuracy: 0.9996 - val_loss: 0.5078 - val_accuracy: 0.9056 - lr: 1.0000e-05\n",
            "Epoch 81/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0945 - accuracy: 0.9996 - val_loss: 0.5112 - val_accuracy: 0.9041 - lr: 1.0000e-05\n",
            "Epoch 82/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0938 - accuracy: 0.9997 - val_loss: 0.5065 - val_accuracy: 0.9049 - lr: 1.0000e-05\n",
            "Epoch 83/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0932 - accuracy: 0.9997 - val_loss: 0.5044 - val_accuracy: 0.9060 - lr: 1.0000e-05\n",
            "Epoch 84/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0927 - accuracy: 0.9997 - val_loss: 0.5061 - val_accuracy: 0.9059 - lr: 1.0000e-05\n",
            "Epoch 85/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0921 - accuracy: 0.9997 - val_loss: 0.5031 - val_accuracy: 0.9070 - lr: 1.0000e-05\n",
            "Epoch 86/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0917 - accuracy: 0.9997 - val_loss: 0.5079 - val_accuracy: 0.9058 - lr: 1.0000e-05\n",
            "Epoch 87/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0911 - accuracy: 0.9998 - val_loss: 0.5062 - val_accuracy: 0.9055 - lr: 1.0000e-05\n",
            "Epoch 88/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0905 - accuracy: 0.9998 - val_loss: 0.5054 - val_accuracy: 0.9057 - lr: 1.0000e-05\n",
            "Epoch 89/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0899 - accuracy: 0.9998 - val_loss: 0.5053 - val_accuracy: 0.9069 - lr: 1.0000e-05\n",
            "Epoch 90/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0891 - accuracy: 0.9999 - val_loss: 0.5038 - val_accuracy: 0.9083 - lr: 1.0000e-05\n",
            "Epoch 91/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0885 - accuracy: 0.9999 - val_loss: 0.5033 - val_accuracy: 0.9064 - lr: 1.0000e-05\n",
            "Epoch 92/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0879 - accuracy: 0.9998 - val_loss: 0.5067 - val_accuracy: 0.9054 - lr: 1.0000e-05\n",
            "Epoch 93/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0873 - accuracy: 0.9998 - val_loss: 0.5031 - val_accuracy: 0.9085 - lr: 1.0000e-05\n",
            "Epoch 94/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0867 - accuracy: 0.9999 - val_loss: 0.4960 - val_accuracy: 0.9082 - lr: 1.0000e-05\n",
            "Epoch 95/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0859 - accuracy: 0.9999 - val_loss: 0.4949 - val_accuracy: 0.9084 - lr: 1.0000e-05\n",
            "Epoch 96/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0852 - accuracy: 0.9999 - val_loss: 0.4970 - val_accuracy: 0.9083 - lr: 1.0000e-05\n",
            "Epoch 97/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0846 - accuracy: 0.9999 - val_loss: 0.4955 - val_accuracy: 0.9073 - lr: 1.0000e-05\n",
            "Epoch 98/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0838 - accuracy: 0.9999 - val_loss: 0.4955 - val_accuracy: 0.9063 - lr: 1.0000e-05\n",
            "Epoch 99/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0833 - accuracy: 0.9998 - val_loss: 0.4989 - val_accuracy: 0.9086 - lr: 1.0000e-05\n",
            "Epoch 100/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0824 - accuracy: 1.0000 - val_loss: 0.5012 - val_accuracy: 0.9081 - lr: 1.0000e-05\n",
            "Epoch 101/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0818 - accuracy: 0.9998 - val_loss: 0.4957 - val_accuracy: 0.9087 - lr: 1.0000e-05\n",
            "Epoch 102/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0816 - accuracy: 0.9999 - val_loss: 0.4979 - val_accuracy: 0.9089 - lr: 1.0000e-06\n",
            "Epoch 103/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0815 - accuracy: 0.9998 - val_loss: 0.4972 - val_accuracy: 0.9088 - lr: 1.0000e-06\n",
            "Epoch 104/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0813 - accuracy: 0.9999 - val_loss: 0.4977 - val_accuracy: 0.9084 - lr: 1.0000e-06\n",
            "Epoch 105/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0812 - accuracy: 0.9999 - val_loss: 0.4979 - val_accuracy: 0.9084 - lr: 1.0000e-06\n",
            "Epoch 106/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0811 - accuracy: 0.9999 - val_loss: 0.4983 - val_accuracy: 0.9078 - lr: 1.0000e-06\n",
            "Epoch 107/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0812 - accuracy: 0.9999 - val_loss: 0.4972 - val_accuracy: 0.9087 - lr: 1.0000e-06\n",
            "Epoch 108/120\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 0.0810 - accuracy: 0.9999 - val_loss: 0.4975 - val_accuracy: 0.9076 - lr: 1.0000e-06\n",
            "Epoch 109/120\n",
            "391/391 [==============================] - 58s 147ms/step - loss: 0.0809 - accuracy: 1.0000 - val_loss: 0.4978 - val_accuracy: 0.9077 - lr: 1.0000e-06\n",
            "Epoch 110/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0808 - accuracy: 0.9999 - val_loss: 0.4980 - val_accuracy: 0.9078 - lr: 1.0000e-06\n",
            "Epoch 111/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0808 - accuracy: 0.9999 - val_loss: 0.4970 - val_accuracy: 0.9080 - lr: 1.0000e-06\n",
            "Epoch 112/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0807 - accuracy: 0.9999 - val_loss: 0.4972 - val_accuracy: 0.9077 - lr: 1.0000e-06\n",
            "Epoch 113/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0806 - accuracy: 0.9999 - val_loss: 0.4957 - val_accuracy: 0.9082 - lr: 1.0000e-06\n",
            "Epoch 114/120\n",
            "391/391 [==============================] - 58s 148ms/step - loss: 0.0805 - accuracy: 0.9999 - val_loss: 0.4966 - val_accuracy: 0.9083 - lr: 1.0000e-06\n",
            "Epoch 115/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0805 - accuracy: 0.9999 - val_loss: 0.4962 - val_accuracy: 0.9079 - lr: 1.0000e-06\n",
            "Epoch 116/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0804 - accuracy: 0.9999 - val_loss: 0.4960 - val_accuracy: 0.9084 - lr: 1.0000e-06\n",
            "Epoch 117/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0803 - accuracy: 0.9999 - val_loss: 0.4965 - val_accuracy: 0.9084 - lr: 1.0000e-06\n",
            "Epoch 118/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0803 - accuracy: 0.9999 - val_loss: 0.4965 - val_accuracy: 0.9078 - lr: 1.0000e-06\n",
            "Epoch 119/120\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 0.0802 - accuracy: 0.9999 - val_loss: 0.4965 - val_accuracy: 0.9073 - lr: 1.0000e-06\n",
            "Epoch 120/120\n",
            "391/391 [==============================] - 58s 148ms/step - loss: 0.0802 - accuracy: 0.9999 - val_loss: 0.4974 - val_accuracy: 0.9076 - lr: 1.0000e-06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}